{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "权重衰退 通过限制参数值的选择范围来控制模型容量\n",
    "最小化损失函数的时候 w的l2范数\n",
    "(w每个项的平方和)小于sita  b不做限制    sita越小正则项越强\n",
    "一般使用均方范数\n",
    "\n",
    "权重衰退会使得某些w项为0(消失)  跟缩小学习率不一样(放慢速度)\n",
    "https://www.cnblogs.com/zzk0/p/15056312.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "# 使过拟合更加明显，设置训练数据集很小,维数很大\n",
    "n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5\n",
    "true_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05\n",
    "train_data = d2l.synthetic_data(true_w, true_b, n_train)\n",
    "train_iter = d2l.load_array(train_data, batch_size)\n",
    "test_data = d2l.synthetic_data(true_w, true_b, n_test)\n",
    "test_iter = d2l.load_array(test_data, batch_size, is_train=False)\n",
    "\n",
    "\n",
    "def init_params():\n",
    "    \"\"\"初始化模型参数\"\"\"\n",
    "    w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True)\n",
    "    b = torch.zeros(1, requires_grad=True)\n",
    "    return [w, b]\n",
    "\n",
    "\n",
    "def l2_penalty(w):\n",
    "    \"\"\"L2范数惩罚\"\"\"\n",
    "    return torch.sum(w.pow(2)) / 2\n",
    "\n",
    "\n",
    "def train(lambd):\n",
    "    '''训练，lambd是L2正则化的超参数'''\n",
    "    w, b = init_params()\n",
    "    net, loss = (\n",
    "        lambda X: d2l.linreg(X, w, b),\n",
    "        d2l.squared_loss,\n",
    "    )  # lambda argument_list:expersion\n",
    "    num_epochs, lr = 100, 0.003\n",
    "    animator = d2l.Animator(\n",
    "        xlabel=\"epochs\",\n",
    "        ylabel=\"loss\",\n",
    "        yscale=\"log\",\n",
    "        xlim=[5, num_epochs],\n",
    "        legend=[\"train\", \"test\"],\n",
    "    )\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in train_iter:\n",
    "            # 增加了L2范数惩罚项，\n",
    "            # 广播机制使l2_penalty(w)成为一个长度为batch_size的向量\n",
    "            l = loss(net(X), y) + lambd * l2_penalty(w)\n",
    "            l.sum().backward()\n",
    "            d2l.sgd([w, b], lr, batch_size)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            animator.add(\n",
    "                epoch + 1,\n",
    "                (\n",
    "                    d2l.evaluate_loss(net, train_iter, loss),\n",
    "                    d2l.evaluate_loss(net, test_iter, loss),\n",
    "                ),\n",
    "            )\n",
    "    print(\"w的L2范数是：\", torch.norm(w).item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 忽略正则化直接训练\n",
    "\n",
    "train(lambd=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用权重衰减   严重过拟合\n",
    "train(lambd=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_concise(wd):\n",
    "    net = nn.Sequential(nn.Linear(num_inputs, 1))\n",
    "    for param in net.parameters():\n",
    "        param.data.normal_()\n",
    "    loss = nn.MSELoss(reduction=\"none\")\n",
    "    num_epochs, lr = 100, 0.003\n",
    "    # 偏置参数没有衰减 L2罚既可以写在目标函数(loss?)，也可以做在训练算法内。在更新w之前，把当前的w乘一个这样的小值。\n",
    "    #这样子做，自动求导少算了? 没理解\n",
    "    trainer = torch.optim.SGD(\n",
    "        [{\"params\": net[0].weight, \"weight_decay\": wd}, {\"params\": net[0].bias}], lr=lr\n",
    "    )\n",
    "    animator = d2l.Animator(\n",
    "        xlabel=\"epochs\",\n",
    "        ylabel=\"loss\",\n",
    "        yscale=\"log\",\n",
    "        xlim=[5, num_epochs],\n",
    "        legend=[\"train\", \"test\"],\n",
    "    )\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            l = loss(net(X), y)\n",
    "            l.sum().backward()\n",
    "            trainer.step()\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            animator.add(\n",
    "                epoch + 1,\n",
    "                (\n",
    "                    d2l.evaluate_loss(net, train_iter, loss),\n",
    "                    d2l.evaluate_loss(net, test_iter, loss),\n",
    "                ),\n",
    "            )\n",
    "    print(\"w的L2范数：\", net[0].weight.norm().item())\n",
    "\n",
    "\n",
    "train_concise(3)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f6bfdd633174b13763f54df811296493377e55ee3e08a58136054b232455b8d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('d2l': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
