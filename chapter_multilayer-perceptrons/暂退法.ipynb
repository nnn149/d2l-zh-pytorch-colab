{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从零实现\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "def dropout_layer(X, dropout):\n",
    "    assert 0 <= dropout <= 1\n",
    "    # 在本情况中，所有元素都被丢弃\n",
    "    if dropout == 1:\n",
    "        return torch.zeros_like(X)\n",
    "    # 在本情况中,所有元素都被保留\n",
    "    if dropout == 0:\n",
    "        return X\n",
    "    # mask 是一个0或者1的张量，为0的概率就是dropout的概率\n",
    "    mask = (torch.rand(X.shape) > dropout).float()  # rand:[0-1] 随机分布； randn:标准正态分布\n",
    "    # mask部位值为1对应位置的元素扩大（1-p）倍，最终X与X\"的期望相同\n",
    "    return mask * X / (1.0 - dropout)  # 这里用 mask*X 是应为做乘法比选元素赋值来的快，有GPU加速\n",
    "\n",
    "\n",
    "# 定义模型参数\n",
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "\n",
    "\n",
    "# 靠近输入层的地方设置较低的暂退概率\n",
    "# 一般就三种取值 0.1 0.5 0.9\n",
    "# 例如隐藏层是64感觉不错，那就可以改隐藏层是128，dropout为0.5，感觉好像等效 64=128*0.5\n",
    "# 应为丢掉一半，可能效果更好？\n",
    "dropout1, dropout2 = 0.2, 0.5\n",
    "\n",
    "# 定义模型\n",
    "class Net(nn.Module):  # 继承nn.Module\n",
    "    def __init__(\n",
    "        self, num_inputs, num_outputs, num_hiddens1, num_hiddens2, is_training=True\n",
    "    ):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.training = is_training\n",
    "        # nn.Linear()自动对参数进行了初始化，是(-\\sqrt(k),\\sqrt(k))的均匀分布，其中k是输入维度的倒数\n",
    "        self.lin1 = nn.Linear(num_inputs, num_hiddens1)  # 隐藏层1\n",
    "        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)  # 隐藏层2\n",
    "        self.lin3 = nn.Linear(num_hiddens2, num_outputs)  # 输出层\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    # nn.module里的__call__方法里面调用到了forward\n",
    "    def forward(self, X):\n",
    "        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))  # 这里展平了输入\n",
    "        # 只有在训练模型时才使用dropout 将暂退法应用于每个隐藏层的输出（在激活函数之后）\n",
    "        # 在计算验证集前，加一句 net.eval()，就会自动关闭dropdout\n",
    "        if self.training == True:\n",
    "            # 在第一个全连接层之后添加一个dropout层\n",
    "            H1 = dropout_layer(H1, dropout1)\n",
    "        H2 = self.relu(self.lin2(H1))\n",
    "        if self.training == True:\n",
    "            # 在第二个全连接层之后添加一个dropout层\n",
    "            H2 = dropout_layer(H2, dropout2)\n",
    "        out = self.lin3(H2)\n",
    "        return out\n",
    "\n",
    "\n",
    "net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练和测试\n",
    "num_epochs, lr, batch_size = 10, 0.5, 256\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n",
    "\n",
    "\n",
    "# 通常模型可以复杂一点，然后通过正则化来控制我们的模型复杂度\n",
    "# 有dropout的情况下，可以把隐藏层稍微设大一点，dropout的率也设大一点\n",
    "# 这样可能比不用dropout，隐藏层小 效果好一些\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f6bfdd633174b13763f54df811296493377e55ee3e08a58136054b232455b8d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('d2l': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
