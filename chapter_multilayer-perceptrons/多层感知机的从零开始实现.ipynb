{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "\n",
    "# 初始化模型参数\n",
    "W1 = nn.Parameter(\n",
    "    torch.randn(num_inputs, num_hiddens, requires_grad=True)\n",
    ")  # shape:784*256\n",
    "b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))  # shape:256\n",
    "\n",
    "W2 = nn.Parameter(\n",
    "    torch.randn(num_hiddens, num_outputs, requires_grad=True) * 0.01\n",
    ")  # shape:256*10\n",
    "\n",
    "b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))  # shape:10\n",
    "params = [W1, b1, W2, b2]\n",
    "\n",
    "\n",
    "def relu(X):\n",
    "    \"\"\"激活函数\"\"\"\n",
    "    a = torch.zeros_like(X)  # 结构相同全为0\n",
    "    return torch.max(X, a)\n",
    "\n",
    "\n",
    "def net(X):\n",
    "    \"\"\"定义模型\"\"\"\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H = relu(X @ W1 + b1)  # @代表矩阵乘法， 线性模型外面套一层relu激活函数\n",
    "    return H @ W2 + b2  # relu后也乘上权重加偏移\n",
    "\n",
    "\n",
    "def CrossEntropyLoss(X, y):\n",
    "    \"\"\"softmax和交叉熵损失函数\"\"\"\n",
    "    X_exp = torch.exp(X)  # X_exp.shape:(batch_size,10)\n",
    "    partition = X_exp.sum(1, keepdim=True)\n",
    "    y_hat = X_exp / partition  # 广播\n",
    "    # y_hat.shape:(batch_size,10) y_hat里面是每个批量中每个类别的概率\n",
    "    # y.shape:(batch_size) y里面是每个批量正确的标号\n",
    "    # 所以用y里的标号取出y_hat中对应标号的正确类别概率\n",
    "    # (若y_hat中取出的概率越大，loss越小\n",
    "    # 这个概率小于1，y=ln(x)在x<1时，y<0，单调增，所以取y的负数使损失值为正数，x=0就是概率为1，损失为0\n",
    "    # 用这个概率做交叉熵损失\n",
    "    # y_hat[range(len(y_hat)), y]  逗号前取出第0个轴对应标号的的内容，也就是每一行，\n",
    "    # 逗号后取出第1轴对应标号的内容，也是就是正确类,结合在一起取出正确类别的预测概率\n",
    "    return -torch.log(y_hat[range(len(y_hat)), y])  # torch.log(X) 就是 数学中的ln(X)\n",
    "\n",
    "\n",
    "def sgd(params, lr, batch_size):\n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    # 这里不需要计算梯度，只是用到了梯度，所以no_grad。训练的时候会对损失函数backward\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size  # 这里为什么要除以batch_size?\n",
    "            param.grad.zero_()\n",
    "\n",
    "\n",
    "def train_epoch(net, train_iter, loss, updater, lr):\n",
    "    \"\"\"训练模型一个迭代周期\"\"\"\n",
    "    # 训练损失总和,训练准确度总和，样本数\n",
    "    metric = d2l.Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        # 计算y_hat和损失梯度，并更新参数\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        l.sum().backward()\n",
    "        updater(params, lr, y.numel())\n",
    "        metric.add(float(l.sum()), d2l.accuracy(y_hat, y), y.numel())\n",
    "    # 返回训练损失和训练精度\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]\n",
    "\n",
    "\n",
    "def train(net, train_iter, test_iter, loss, num_epochs, updater, lr):\n",
    "    \"\"\"训练模型\"\"\"\n",
    "    # 画图\n",
    "    animator = d2l.Animator(\n",
    "        xlabel=\"epoch\",\n",
    "        xlim=[1, num_epochs],\n",
    "        ylim=[0.3, 0.9],\n",
    "        legend=[\"train loss\", \"train acc\", \"test acc\"],\n",
    "    )\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch(net, train_iter, loss, updater, lr)\n",
    "        test_acc = d2l.evaluate_accuracy(net, test_iter)\n",
    "        animator.add(epoch + 1, train_metrics + (test_acc,))\n",
    "\n",
    "\n",
    "# 训练\n",
    "num_epochs, lr = 1, 0.1  # 训练次数,学习率\n",
    "\n",
    "train(net, train_iter, test_iter, CrossEntropyLoss, num_epochs, sgd, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def relu(X):\n",
    "    \"\"\"激活函数\"\"\"\n",
    "    a = torch.zeros_like(X)  # 结构相同全为0\n",
    "    return torch.max(X, a)\n",
    "\n",
    "\n",
    "def net(X):\n",
    "    \"\"\"定义模型\"\"\"\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H = relu(X @ W1 + b1)  # @代表矩阵乘法， 线性模型外面套一层relu激活函数\n",
    "    return H @ W2 + b2  # relu后也乘上权重加偏移\n",
    "\n",
    "\n",
    "def CrossEntropyLoss(X, y):\n",
    "    \"\"\"softmax和交叉熵损失函数\"\"\"\n",
    "    X_exp = torch.exp(X)  # X_exp.shape:(batch_size,10)\n",
    "    partition = X_exp.sum(1, keepdim=True)\n",
    "    y_hat = X_exp / partition  # 广播\n",
    "    # y_hat.shape:(batch_size,10) y_hat里面是每个批量中每个类别的概率\n",
    "    # y.shape:(batch_size) y里面是每个批量正确的标号\n",
    "    # 所以用y里的标号取出y_hat中对应标号的正确类别概率\n",
    "    # (若y_hat中取出的概率越大，loss越小\n",
    "    # 这个概率小于1，y=ln(x)在x<1时，y<0，单调增，所以取y的负数使损失值为正数，x=0就是概率为1，损失为0\n",
    "    # 用这个概率做交叉熵损失\n",
    "    # y_hat[range(len(y_hat)), y]  逗号前取出第0个轴对应标号的的内容，也就是每一行，\n",
    "    # 逗号后取出第1轴对应标号的内容，也是就是正确类,结合在一起取出正确类别的预测概率\n",
    "    return -torch.log(y_hat[range(len(y_hat)), y])  # torch.log(X) 就是 数学中的ln(X)\n",
    "\n",
    "\n",
    "def sgd(params, lr, batch_size):\n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    # 这里不需要计算梯度，只是用到了梯度，所以no_grad。训练的时候会对损失函数backward\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size  # 这里为什么要除以batch_size?\n",
    "            param.grad.zero_()\n",
    "\n",
    "\n",
    "def train_epoch(net, train_iter, loss, updater, lr):\n",
    "    \"\"\"训练模型一个迭代周期\"\"\"\n",
    "    # 训练损失总和,训练准确度总和，样本数\n",
    "    metric = d2l.Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        # 计算y_hat和损失梯度，并更新参数\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        l.sum().backward()\n",
    "        updater(params, lr, y.numel())\n",
    "        metric.add(float(l.sum()), d2l.accuracy(y_hat, y), y.numel())\n",
    "    # 返回训练损失和训练精度\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]\n",
    "\n",
    "\n",
    "def train(net, train_iter, test_iter, loss, num_epochs, updater, lr):\n",
    "    \"\"\"训练模型\"\"\"\n",
    "    # 画图\n",
    "    animator = d2l.Animator(\n",
    "        xlabel=\"epoch\",\n",
    "        xlim=[1, num_epochs],\n",
    "        ylim=[0.3, 0.9],\n",
    "        legend=[\"train loss\", \"train acc\", \"test acc\"],\n",
    "    )\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch(net, train_iter, loss, updater, lr)\n",
    "        test_acc = d2l.evaluate_accuracy(net, test_iter)\n",
    "        animator.add(epoch + 1, train_metrics + (test_acc,))\n",
    "\n",
    "\n",
    "# 训练\n",
    "num_epochs, lr = 1, 0.1  # 训练次数,学习率\n",
    "\n",
    "train(net, train_iter, test_iter, CrossEntropyLoss, num_epochs, sgd, lr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
